# Recipe AI — MVP Architecture Context (Supabase Queue + R2 + Hibernating Worker)

> **Goal:** Decouple transcription from the synchronous backend, keep MVP costs *very low*, and keep the codebase **modular** so we can later swap the queue (e.g., RabbitMQ) and/or execution (Cloud Run GPU, GKE, SaaS STT) with minimal refactor.

## Current state (given)
- The system is **already functional** and runs **fast-whisper locally**.
- Web frontend (**React + TypeScript**) is working.
- Mobile (**Flutter**) is **not started** yet.
- Current implementation is **coupled** (services not separated into independent workers).
- Supabase is already used as the main DB (including vectors for RAG). Some tables exist and should be reviewed and adapted with minimal changes.

---

## Target MVP architecture (cost-optimized, robust enough)

### Components
1. **Web Frontend (React + TS)**
   - Uses **Signed URL** to upload media directly to **Cloudflare R2**.
   - Creates a transcription job in the API.
   - Polls job status (MVP). Optional later: Supabase Realtime or FCM push.

2. **Backend API (Python / FastAPI)**
   - Stateless.
   - Responsibilities:
     - Auth validation (keep current auth now; optional future: Firebase Auth).
     - Generate **R2 Signed PUT URLs**.
     - Create/validate jobs in **Supabase Postgres** (queue-in-DB).
     - Apply quotas / limits (minutes/day and/or jobs/day).
     - Provide job status/results endpoints.
     - (Optional later) send notifications (FCM).

3. **Queue + DB: Supabase Postgres**
   - Uses a **jobs table** as a durable queue.
   - Uses **row locking** (`FOR UPDATE SKIP LOCKED`) for safe multi-worker concurrency.
   - Stores transcription results and metadata.
   - Stores **usage/quota** info.

4. **Storage: Cloudflare R2 (S3-compatible)**
   - Holds uploaded media temporarily.
   - Aggressive lifecycle rule (e.g., delete raw media after 7–14 days).
   - Egress-friendly for workers outside a single cloud.

5. **Transcription Worker (hibernating VM / local now)**
   - Runs `faster-whisper` (fast-whisper) to transcribe.
   - Polls Supabase for queued jobs; locks one; downloads from R2; transcribes; writes results; updates status.
   - For MVP cost: run on **a cheap VM that is off most of the time** (hibernates). Start it on a schedule window or on-demand.

---

## Job lifecycle and flow

### Upload + job creation (client-driven)
1. **Client → API**: `POST /media/signed-upload`
2. **API → Client**: returns:
   - `object_key` (generated by backend)
   - `upload_url` (signed PUT)
   - expiration
3. **Client → R2**: uploads media via `upload_url`
4. **Client → API**: `POST /transcriptions/jobs` with `object_key` (+ recipe_id optional)
5. **API → Supabase**: inserts job with `status='QUEUED'` (after quota check)

### Processing (worker-driven)
6. **Worker**: fetch next `QUEUED` job using `FOR UPDATE SKIP LOCKED`
7. **Worker**:
   - sets `status='RUNNING'`, `locked_at`, `locked_by`, `started_at`
   - downloads file from R2
   - runs `faster-whisper`
   - writes results (text + segments)
   - sets `status='DONE'` (or `FAILED` with retries/backoff)
8. **Client** polls `GET /transcriptions/jobs/{job_id}` until DONE

---

## Why Supabase/Postgres queue (for MVP)
- **Minimum moving parts**: no extra infra for Redis/RabbitMQ now.
- **Durable**: jobs persist, results centralized, easy to inspect.
- **Robust enough** if implemented with:
  - `FOR UPDATE SKIP LOCKED`
  - lock TTL / reaper
  - retry with backoff
  - idempotency

**Planned migration later**: Swap queue adapter to **RabbitMQ** (or NATS/Temporal) with minimal code changes by keeping adapters and interfaces now.

---

## Modularity requirements (so future RabbitMQ migration is easy)

### Core interfaces (must exist)
- `StorageProvider`
  - `generate_signed_put_url(object_key, content_type, expires_seconds) -> str`
  - `download_to_path(object_key, target_path) -> Path`
  - (optional) `generate_signed_get_url(...)`

- `JobQueueRepository` (queue abstraction)
  - `enqueue_transcription_job(user_id, object_key, recipe_id, estimated_duration_sec, ...) -> job_id`
  - `fetch_and_lock_next_job(worker_id, now_ts) -> Job | None`
  - `mark_running(job_id, worker_id, ...)`
  - `mark_done(job_id, result, ...)`
  - `mark_failed(job_id, error, retry_at, ...)`
  - `release_or_requeue_stale_locks(lock_ttl_minutes)`

- `QuotaService`
  - `check_and_reserve_minutes(user_id, estimated_minutes) -> allowed/denied`
  - (optional) confirm actual minutes after processing

### Worker orchestration
- `TranscriptionPipeline` (pure business logic)
  - `transcribe(media_path) -> {text, segments, language, duration_sec}`
- The worker should only orchestrate: **fetch job → download → pipeline → save**.

---

## Suggested repository structure (minimum disruption)

```
backend/
  app/
    api/
      routes/
        media.py
        transcriptions.py
    domain/
      models.py        # JobStatus enum, dataclasses
      errors.py
    services/
      quota_service.py
      transcription_pipeline.py
    infra/
      storage/
        r2_provider.py
      db/
        supabase_jobs_repo.py
  workers/
    transcriber/
      main.py          # polling loop + locking + processing
      config.py
      logging.py
  migrations/
    001_jobs.sql
    002_usage_daily.sql
```

> Keep the existing code working by introducing new routes under `/v2` or behind a feature flag until the new flow is validated.

---

## Supabase schema guidance (adapt to existing tables)
> We will **review existing tables** and adapt with **minimal changes**. If you paste the current schema/DDL, we will produce a minimal migration diff.

### `transcription_jobs` (queue + results)
Required columns (recommended):
- `id uuid pk`
- `user_id uuid not null`
- `recipe_id uuid null`
- `object_key text not null` *(or media_asset_id FK)*
- `status text not null` *(QUEUED/RUNNING/DONE/FAILED/CANCELLED)*
- `priority int default 0`
- `locked_at timestamptz null`
- `locked_by text null`
- `attempt_count int default 0`
- `max_attempts int default 3`
- `next_attempt_at timestamptz null`
- `error_message text null`
- `created_at timestamptz default now()`
- `started_at timestamptz null`
- `finished_at timestamptz null`
- **results**
  - `duration_sec int null`
  - `language text null`
  - `transcript_text text null`
  - `segments_json jsonb null`
  - `model_version text null`

Indexes:
- `(status, next_attempt_at)`
- `(user_id, created_at desc)`
- `(locked_at)`

### `usage_daily` (cost control)
- `user_id uuid`
- `date date`
- `minutes_used int default 0`
- `jobs_count int default 0`
- `updated_at timestamptz default now()`
Primary key `(user_id, date)`

**Quota strategy (MVP)**
- Limit by **minutes/day** (not jobs/day).
- Example: 30–60 minutes/day/user.
- On job creation: reserve estimated minutes.
- On completion: optionally reconcile with actual duration.

---

## Worker reliability details (must implement)

### Locking pattern (Postgres)
- Fetch job:
  - `WHERE status='QUEUED' AND (next_attempt_at IS NULL OR next_attempt_at <= now())`
  - ordered by `priority desc, created_at asc`
  - `FOR UPDATE SKIP LOCKED`
- Update to RUNNING in the same transaction.

### Stale locks / crash recovery
- If `status='RUNNING'` and `locked_at < now() - TTL`, requeue:
  - set back to `QUEUED`
  - clear lock
  - increment attempt (optional) and set `next_attempt_at`

### Retries with backoff
- transient failure → `attempt_count += 1`, `next_attempt_at = now() + backoff`
- if `attempt_count >= max_attempts` → `FAILED`

### Idempotency
- Job `id` is the idempotency key.
- Worker must never create a duplicate result row for the same job.
- Results stored on the job itself (simplest for MVP).

---

## R2 implementation notes (S3-compatible)
- Use AWS S3 SDK (boto3) pointing to the R2 endpoint.
- Use pre-signed PUT URLs with content-type restrictions if possible.
- Generate `object_key` server-side:
  - `users/{user_id}/media/{yyyy}/{mm}/{uuid}.{ext}`
- Apply lifecycle rules on the bucket to delete raw media after 7–14 days.

---

## Hibernating worker VM strategy (MVP cost win)


### On-demand (better UX)
- When a job is enqueued, trigger VM start (small automation).
- Worker stops VM after queue is empty for N minutes.

> Implementation of start/stop can be postponed; for MVP you can run locally or keep a VM on only during your active testing windows.

---

## Future migration: RabbitMQ (planned)
When you scale and need:
- dedicated DLQ/redrive tooling
- high volume spikes
- multi-step workflows
- autoscaling based on backlog

**Because we are modular now**, migrating means:
- Implement `RabbitJobQueueRepository` behind the same interface.
- Keep `TranscriptionPipeline`, `StorageProvider`, and DB result writes the same.
- Keep API contracts unchanged.

---

## Implementation plan (step-by-step)

### Phase 0 — Inventory & safety
1. List existing Supabase tables related to recipes/media/jobs and map what can be reused.
2. Add feature flag or `/v2` endpoints to avoid breaking current flow.

### Phase 1 — Storage decoupling (R2)
3. Implement `R2StorageProvider`.
4. Add `POST /media/signed-upload` returning `object_key + upload_url`.
5. Update web to upload directly to R2 via signed PUT.

### Phase 2 — Queue in Supabase
6. Create/adjust `transcription_jobs` schema + indexes.
7. Implement `JobQueueRepository` using Supabase/Postgres.
8. Add `POST /transcriptions/jobs` and `GET /transcriptions/jobs/{id}`.

### Phase 3 — Worker split
9. Create `workers/transcriber/main.py`:
   - polling loop
   - fetch+lock
   - download from R2
   - run faster-whisper
   - update job DONE/FAILED
10. Keep old synchronous path temporarily; route new UI to v2.

### Phase 4 — Cost controls
11. Implement `usage_daily` + quota checks on job creation.
12. Add lifecycle policy on R2 bucket.

### Phase 5 — Production hardening
13. Logging/metrics around:
   - queue wait time
   - transcription time
   - failure rate
14. Add stale-lock reaper (can run inside worker at startup/interval).

---

## Deliverables expected from agents (Cursor/Trae/Codex/Claude)
- New modules with interfaces (storage, queue repo, pipeline).
- New API routes (`/media/signed-upload`, `/transcriptions/jobs`, `/transcriptions/jobs/{id}`).
- Worker entrypoint and minimal config.
- SQL migrations (or Supabase migration scripts).
- Minimal web changes for signed upload + job polling.
- Notes on how to run locally + environment variables.
